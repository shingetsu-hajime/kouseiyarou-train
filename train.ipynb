{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "id": "13L45ApiGXNa"
            },
            "outputs": [],
            "source": [
                "# 9-2\n",
                "# !pip install transformers==4.18.0 fugashi==1.1.0 ipadic==1.0.0 pytorch-lightning==1.6.1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "id": "0ULhAjFPGXNa"
            },
            "outputs": [],
            "source": [
                "# 9-3\n",
                "import random\n",
                "from tqdm import tqdm\n",
                "import unicodedata\n",
                "\n",
                "import pandas as pd\n",
                "import torch\n",
                "from torch.utils.data import DataLoader\n",
                "from transformers import BertJapaneseTokenizer, BertForMaskedLM\n",
                "import pytorch_lightning as pl\n",
                "\n",
                "# 日本語の事前学習済みモデル\n",
                "MODEL_NAME = 'tohoku-nlp/bert-base-japanese-whole-word-masking'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "id": "MC1yL-SSGXNb"
            },
            "outputs": [],
            "source": [
                "# 9-4\n",
                "class SC_tokenizer(BertJapaneseTokenizer):\n",
                "       \n",
                "    def encode_plus_tagged(\n",
                "        self, wrong_text, correct_text, max_length=128\n",
                "    ):\n",
                "        \"\"\"\n",
                "        ファインチューニング時に使用。\n",
                "        誤変換を含む文章と正しい文章を入力とし、\n",
                "        符号化を行いBERTに入力できる形式にする。\n",
                "        \"\"\"\n",
                "        # 誤変換した文章をトークン化し、符号化\n",
                "        encoding = self(\n",
                "            wrong_text, \n",
                "            max_length=max_length, \n",
                "            padding='max_length', \n",
                "            truncation=True\n",
                "        )\n",
                "        # 正しい文章をトークン化し、符号化\n",
                "        encoding_correct = self(\n",
                "            correct_text,\n",
                "            max_length=max_length,\n",
                "            padding='max_length',\n",
                "            truncation=True\n",
                "        ) \n",
                "        # 正しい文章の符号をラベルとする\n",
                "        encoding['labels'] = encoding_correct['input_ids'] \n",
                "\n",
                "        return encoding\n",
                "\n",
                "    def encode_plus_untagged(\n",
                "        self, text, max_length=None, return_tensors=None\n",
                "    ):\n",
                "        \"\"\"\n",
                "        文章を符号化し、それぞれのトークンの文章中の位置も特定しておく。\n",
                "        \"\"\"\n",
                "        # 文章のトークン化を行い、\n",
                "        # それぞれのトークンと文章中の文字列を対応づける。\n",
                "        tokens = [] # トークンを追加していく。\n",
                "        tokens_original = [] # トークンに対応する文章中の文字列を追加していく。\n",
                "        words = self.word_tokenizer.tokenize(text) # MeCabで単語に分割\n",
                "        for word in words:\n",
                "            # 単語をサブワードに分割\n",
                "            tokens_word = self.subword_tokenizer.tokenize(word) \n",
                "            tokens.extend(tokens_word)\n",
                "            if tokens_word[0] == '[UNK]': # 未知語への対応\n",
                "                tokens_original.append(word)\n",
                "            else:\n",
                "                tokens_original.extend([\n",
                "                    token.replace('##','') for token in tokens_word\n",
                "                ])\n",
                "\n",
                "        # 各トークンの文章中での位置を調べる。（空白の位置を考慮する）\n",
                "        position = 0\n",
                "        spans = [] # トークンの位置を追加していく。\n",
                "        for token in tokens_original:\n",
                "            l = len(token)\n",
                "            while 1:\n",
                "                if token != text[position:position+l]:\n",
                "                    position += 1\n",
                "                else:\n",
                "                    spans.append([position, position+l])\n",
                "                    position += l\n",
                "                    break\n",
                "\n",
                "        # 符号化を行いBERTに入力できる形式にする。\n",
                "        input_ids = self.convert_tokens_to_ids(tokens) \n",
                "        encoding = self.prepare_for_model(\n",
                "            input_ids, \n",
                "            max_length=max_length, \n",
                "            padding='max_length' if max_length else False, \n",
                "            truncation=True if max_length else False\n",
                "        )\n",
                "        sequence_length = len(encoding['input_ids'])\n",
                "        # 特殊トークン[CLS]に対するダミーのspanを追加。\n",
                "        spans = [[-1, -1]] + spans[:sequence_length-2] \n",
                "        # 特殊トークン[SEP]、[PAD]に対するダミーのspanを追加。\n",
                "        spans = spans + [[-1, -1]] * ( sequence_length - len(spans) ) \n",
                "\n",
                "        # 必要に応じてtorch.Tensorにする。\n",
                "        if return_tensors == 'pt':\n",
                "            encoding = { k: torch.tensor([v]) for k, v in encoding.items() }\n",
                "\n",
                "        return encoding, spans\n",
                "\n",
                "    def convert_bert_output_to_text(self, text, labels, spans):\n",
                "        \"\"\"\n",
                "        推論時に使用。\n",
                "        文章と、各トークンのラベルの予測値、文章中での位置を入力とする。\n",
                "        そこから、BERTによって予測された文章に変換。\n",
                "        \"\"\"\n",
                "        assert len(spans) == len(labels)\n",
                "\n",
                "        # labels, spansから特殊トークンに対応する部分を取り除く\n",
                "        labels = [label for label, span in zip(labels, spans) if span[0]!=-1]\n",
                "        spans = [span for span in spans if span[0]!=-1]\n",
                "\n",
                "        # BERTが予測した文章を作成\n",
                "        predicted_text = ''\n",
                "        position = 0\n",
                "        for label, span in zip(labels, spans):\n",
                "            start, end = span\n",
                "            if position != start: # 空白の処理\n",
                "                predicted_text += text[position:start]\n",
                "            predicted_token = self.convert_ids_to_tokens(label)\n",
                "            predicted_token = predicted_token.replace('##', '')\n",
                "            predicted_token = unicodedata.normalize(\n",
                "                'NFKC', predicted_token\n",
                "            ) \n",
                "            predicted_text += predicted_token\n",
                "            position = end\n",
                "        \n",
                "        return predicted_text"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "id": "HBTlINuoGXNc"
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "23814878d4b14db58e9599b4ec1dc6f3",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer_config.json:   0%|          | 0.00/120 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "ba70677778c543c698c6473c274a8ff0",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "vocab.txt:   0%|          | 0.00/258k [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "de26de9a921a4b7393a109da386f693e",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "config.json:   0%|          | 0.00/479 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
                        "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
                        "The class this function is called from is 'SC_tokenizer'.\n"
                    ]
                }
            ],
            "source": [
                "# 9-5\n",
                "tokenizer = SC_tokenizer.from_pretrained(MODEL_NAME)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {
                "id": "e27NIdKYGXNc"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{'input_ids': [2, 759, 18204, 11, 4618, 15, 10, 3, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0], 'labels': [2, 759, 18204, 11, 8274, 15, 10, 3, 0, 0, 0, 0]}\n"
                    ]
                }
            ],
            "source": [
                "# 9-6\n",
                "wrong_text = '優勝トロフィーを変換した'\n",
                "correct_text = '優勝トロフィーを返還した'\n",
                "encoding = tokenizer.encode_plus_tagged(\n",
                "    wrong_text, correct_text, max_length=12\n",
                ")\n",
                "print(encoding)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {
                "id": "rLBWoRIcGXNd"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "# encoding\n",
                        "{'input_ids': tensor([[    2,   759, 18204,    11,  4618,    15,    10,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
                        "# spans\n",
                        "[[-1, -1], [0, 2], [2, 7], [7, 8], [8, 10], [10, 11], [11, 12], [-1, -1]]\n"
                    ]
                }
            ],
            "source": [
                "# 9-7\n",
                "wrong_text = '優勝トロフィーを変換した'\n",
                "encoding, spans = tokenizer.encode_plus_untagged(\n",
                "    wrong_text, return_tensors='pt'\n",
                ")\n",
                "print('# encoding')\n",
                "print(encoding)\n",
                "print('# spans')\n",
                "print(spans)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {
                "id": "O7gH4oB9GXNd"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "優勝トロフィーを返還した\n"
                    ]
                }
            ],
            "source": [
                "# 9-8\n",
                "predicted_labels = [2, 759, 18204, 11, 8274, 15, 10, 3]\n",
                "predicted_text = tokenizer.convert_bert_output_to_text(\n",
                "    wrong_text, predicted_labels, spans\n",
                ")\n",
                "print(predicted_text)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {
                "id": "zCFGE4xbGXNe",
                "jupyter": {
                    "outputs_hidden": false
                }
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "05caa8bf00314001bc914d4e3b3552a8",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "pytorch_model.bin:   0%|          | 0.00/445M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Some weights of the model checkpoint at tohoku-nlp/bert-base-japanese-whole-word-masking were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
                        "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
                        "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
                    ]
                }
            ],
            "source": [
                "# 9-9\n",
                "bert_mlm = BertForMaskedLM.from_pretrained(MODEL_NAME)\n",
                "bert_mlm = bert_mlm.cuda()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {
                "id": "J2DR-ls8GXNf"
            },
            "outputs": [],
            "source": [
                "# 9-10\n",
                "text = '優勝トロフィーを変換した。'\n",
                "\n",
                "# 符号化とともに各トークンの文章中の位置を計算しておく。\n",
                "encoding, spans = tokenizer.encode_plus_untagged(\n",
                "    text, return_tensors='pt'\n",
                ")\n",
                "encoding = { k: v.cuda() for k, v in encoding.items() }\n",
                "\n",
                "# BERTに入力し、トークン毎にスコアの最も高いトークンのIDを予測値とする。\n",
                "with torch.no_grad():\n",
                "    output = bert_mlm(**encoding)\n",
                "    scores = output.logits\n",
                "    labels_predicted = scores[0].argmax(-1).cpu().numpy().tolist()\n",
                "    \n",
                "# ラベル列を文章に変換\n",
                "predict_text = tokenizer.convert_bert_output_to_text(\n",
                "    text, labels_predicted, spans\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {
                "id": "Wc0mIVlnGXNf"
            },
            "outputs": [],
            "source": [
                "# 9-11\n",
                "data = [\n",
                "    {\n",
                "        'wrong_text': '優勝トロフィーを変換した。',\n",
                "        'correct_text': '優勝トロフィーを返還した。',\n",
                "    },\n",
                "    {\n",
                "        'wrong_text': '人と森は強制している。',\n",
                "        'correct_text': '人と森は共生している。',\n",
                "    }\n",
                "]\n",
                "\n",
                "# 各データを符号化し、データローダへ入力できるようにする。\n",
                "max_length=32\n",
                "dataset_for_loader = []\n",
                "for sample in data:\n",
                "    wrong_text = sample['wrong_text']\n",
                "    correct_text = sample['correct_text']\n",
                "    encoding = tokenizer.encode_plus_tagged(\n",
                "        wrong_text, correct_text, max_length=max_length\n",
                "    )\n",
                "    encoding = { k: torch.tensor(v) for k, v in encoding.items() }\n",
                "    dataset_for_loader.append(encoding)\n",
                "\n",
                "# データローダを作成\n",
                "dataloader = DataLoader(dataset_for_loader, batch_size=2)\n",
                "\n",
                "# ミニバッチをBERTへ入力し、損失を計算。\n",
                "for batch in dataloader:\n",
                "    encoding = { k: v.cuda() for k, v in batch.items() }\n",
                "    output = bert_mlm(**encoding)\n",
                "    loss = output.loss # 損失"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "V0KBJLE4GXNg"
            },
            "outputs": [],
            "source": [
                "# # 9-12\n",
                "# !curl -L \"https://nlp.ist.i.kyoto-u.ac.jp/DLcounter/lime.cgi?down=https://nlp.ist.i.kyoto-u.ac.jp/nl-resource/JWTD/jwtd.tar.gz&name=JWTD.tar.gz\" -o JWTD.tar.gz\n",
                "# !tar zxvf JWTD.tar.gz"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {
                "id": "Z-lflu5sGXNg",
                "jupyter": {
                    "outputs_hidden": false
                }
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
                        "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
                        "The class this function is called from is 'SC_tokenizer'.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "学習と検証用のデータセット：\n",
                        "- 漢字誤変換の総数：696189\n",
                        "- トークンの対応関係のつく文章の総数: 308555\n",
                        "  (全体の44%)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
                        "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
                        "The class this function is called from is 'SC_tokenizer'.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "テスト用のデータセット：\n",
                        "- 漢字誤変換の総数：5440\n",
                        "- トークンの対応関係のつく文章の総数: 2249\n",
                        "  (全体の41%)\n"
                    ]
                }
            ],
            "source": [
                "# 9-13\n",
                "def create_dataset(data_df):\n",
                "\n",
                "    tokenizer = SC_tokenizer.from_pretrained(MODEL_NAME)\n",
                "\n",
                "    def check_token_count(row):\n",
                "        \"\"\"\n",
                "        誤変換の文章と正しい文章でトークンに対応がつくかどうかを判定。\n",
                "        （条件は上の文章を参照）\n",
                "        \"\"\"\n",
                "        wrong_text_tokens = tokenizer.tokenize(row['wrong_text'])\n",
                "        correct_text_tokens = tokenizer.tokenize(row['correct_text'])\n",
                "        if len(wrong_text_tokens) != len(correct_text_tokens):\n",
                "            return False\n",
                "        \n",
                "        diff_count = 0\n",
                "        threthold_count = 2\n",
                "        for wrong_text_token, correct_text_token \\\n",
                "            in zip(wrong_text_tokens, correct_text_tokens):\n",
                "\n",
                "            if wrong_text_token != correct_text_token:\n",
                "                diff_count += 1\n",
                "                if diff_count > threthold_count:\n",
                "                    return False\n",
                "        return True\n",
                "\n",
                "    def normalize(text):\n",
                "        \"\"\"\n",
                "        文字列の正規化\n",
                "        \"\"\"\n",
                "        text = text.strip()\n",
                "        text = unicodedata.normalize('NFKC', text)\n",
                "        return text\n",
                "\n",
                "    # 漢字の誤変換のデータのみを抜き出す。\n",
                "    # category_type = 'kanji-conversion'\n",
                "    # data_df.query('category == @category_type', inplace=True) \n",
                "    data_df.rename(\n",
                "        columns={'pre_text': 'wrong_text', 'post_text': 'correct_text'}, \n",
                "        inplace=True\n",
                "    )\n",
                "    \n",
                "    # 誤変換と正しい文章をそれぞれ正規化し、\n",
                "    # それらの間でトークン列に対応がつくもののみを抜き出す。\n",
                "    data_df['wrong_text'] = data_df['wrong_text'].map(normalize) \n",
                "    data_df['correct_text'] = data_df['correct_text'].map(normalize)\n",
                "    kanji_conversion_num = len(data_df)\n",
                "    data_df = data_df[data_df.apply(check_token_count, axis=1)]\n",
                "    same_tokens_count_num = len(data_df)\n",
                "    print(\n",
                "        f'- 漢字誤変換の総数：{kanji_conversion_num}',\n",
                "        f'- トークンの対応関係のつく文章の総数: {same_tokens_count_num}',\n",
                "        f'  (全体の{same_tokens_count_num/kanji_conversion_num*100:.0f}%)',\n",
                "        sep = '\\n'\n",
                "    )\n",
                "    return data_df[['wrong_text', 'correct_text']].to_dict(orient='records')\n",
                "\n",
                "# データのロード\n",
                "train_df = pd.read_json(\n",
                "    './data/jwtd_v2.0/train.jsonl', orient='records', lines=True\n",
                ")\n",
                "test_df = pd.read_json(\n",
                "    './data/jwtd_v2.0/test.jsonl', orient='records', lines=True\n",
                ")\n",
                "\n",
                "# 学習用と検証用データ\n",
                "print('学習と検証用のデータセット：')\n",
                "dataset = create_dataset(train_df)\n",
                "random.shuffle(dataset)\n",
                "n = len(dataset)\n",
                "n_train = int(n*0.8)\n",
                "dataset_train = dataset[:n_train]\n",
                "dataset_val = dataset[n_train:]\n",
                "\n",
                "# テストデータ\n",
                "print('テスト用のデータセット：')\n",
                "dataset_test = create_dataset(test_df)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {
                "id": "hlr9SXDNGXNh"
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
                        "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
                        "The class this function is called from is 'SC_tokenizer'.\n",
                        "100%|██████████| 246844/246844 [01:23<00:00, 2971.22it/s]\n",
                        "100%|██████████| 61711/61711 [00:21<00:00, 2925.41it/s]\n"
                    ]
                }
            ],
            "source": [
                "# 9-14\n",
                "def create_dataset_for_loader(tokenizer, dataset, max_length):\n",
                "    \"\"\"\n",
                "    データセットをデータローダに入力可能な形式にする。\n",
                "    \"\"\"\n",
                "    dataset_for_loader = []\n",
                "    for sample in tqdm(dataset):\n",
                "        wrong_text = sample['wrong_text']\n",
                "        correct_text = sample['correct_text']\n",
                "        encoding = tokenizer.encode_plus_tagged(\n",
                "            wrong_text, correct_text, max_length=max_length\n",
                "        )\n",
                "        encoding = { k: torch.tensor(v) for k, v in encoding.items() }\n",
                "        dataset_for_loader.append(encoding)\n",
                "    return dataset_for_loader\n",
                "\n",
                "tokenizer = SC_tokenizer.from_pretrained(MODEL_NAME)\n",
                "\n",
                "# データセットの作成\n",
                "max_length = 32\n",
                "dataset_train_for_loader = create_dataset_for_loader(\n",
                "    tokenizer, dataset_train, max_length\n",
                ")\n",
                "dataset_val_for_loader = create_dataset_for_loader(\n",
                "    tokenizer, dataset_val, max_length\n",
                ")\n",
                "\n",
                "# データローダの作成\n",
                "dataloader_train = DataLoader(\n",
                "    dataset_train_for_loader, batch_size=32, shuffle=True\n",
                ")\n",
                "dataloader_val = DataLoader(dataset_val_for_loader, batch_size=256)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "metadata": {
                "id": "c-j7R6z0GXNh"
            },
            "outputs": [],
            "source": [
                "# 9-15\n",
                "# 量子化エンジンの設定\n",
                "# torch.backends.quantized.engine = 'qnnpack'\n",
                "\n",
                "class BertForMaskedLM_pl(pl.LightningModule):\n",
                "    \n",
                "    def __init__(self, model_name, lr):\n",
                "        super().__init__()\n",
                "        self.save_hyperparameters()\n",
                "        self.bert_mlm = BertForMaskedLM.from_pretrained(model_name)\n",
                "        \n",
                "    def training_step(self, batch, batch_idx):\n",
                "        output = self.bert_mlm(**batch)\n",
                "        loss = output.loss\n",
                "        self.log('train_loss', loss)\n",
                "        return loss\n",
                "        \n",
                "    def validation_step(self, batch, batch_idx):\n",
                "        output = self.bert_mlm(**batch)\n",
                "        val_loss = output.loss\n",
                "        self.log('val_loss', val_loss)\n",
                "   \n",
                "    def configure_optimizers(self):\n",
                "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "checkpoint = pl.callbacks.ModelCheckpoint(\n",
                "    monitor='val_loss',\n",
                "    mode='min',\n",
                "    save_top_k=1,\n",
                "    save_weights_only=True,\n",
                "    dirpath='model/'\n",
                ")\n",
                "\n",
                "trainer = pl.Trainer(\n",
                "    max_epochs=5,\n",
                "    callbacks=[checkpoint]\n",
                ")\n",
                "\n",
                "# ファインチューニング\n",
                "model = BertForMaskedLM_pl(MODEL_NAME, lr=1e-5)\n",
                "trainer.fit(model, dataloader_train, dataloader_val)\n",
                "best_model_path = checkpoint.best_model_path"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {
                "id": "w2eNTz6OsvVP"
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
                        "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
                        "The class this function is called from is 'SC_tokenizer'.\n",
                        "Some weights of the model checkpoint at tohoku-nlp/bert-base-japanese-whole-word-masking were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
                        "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
                        "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "---\n",
                        "入力：ユーザーの試行に合わせた楽曲を配信する。\n",
                        "出力：ユーザーの嗜好に合わせた楽曲を配信する。\n",
                        "---\n",
                        "入力：メールに明日の会議の史料を添付した。\n",
                        "出力：メールに明日の会議の資料を添付した。\n",
                        "---\n",
                        "入力：乳酸菌で牛乳を発行するとヨーグルトができる。\n",
                        "出力：乳酸菌で牛乳を発酵するとヨーグルトができる。\n",
                        "---\n",
                        "入力：突然、子供が帰省を発した。\n",
                        "出力：突然、子供が帰省を発した。\n"
                    ]
                }
            ],
            "source": [
                "# 9-16\n",
                "def predict(text, tokenizer, bert_mlm):\n",
                "    \"\"\"\n",
                "    文章を入力として受け、BERTが予測した文章を出力\n",
                "    \"\"\"\n",
                "    # 符号化\n",
                "    encoding, spans = tokenizer.encode_plus_untagged(\n",
                "        text, return_tensors='pt'\n",
                "    ) \n",
                "    encoding = { k: v.cuda() for k, v in encoding.items() }\n",
                "\n",
                "    # ラベルの予測値の計算\n",
                "    with torch.no_grad():\n",
                "        output = bert_mlm(**encoding)\n",
                "        scores = output.logits\n",
                "        labels_predicted = scores[0].argmax(-1).cpu().numpy().tolist()\n",
                "\n",
                "    # ラベル列を文章に変換\n",
                "    predict_text = tokenizer.convert_bert_output_to_text(\n",
                "        text, labels_predicted, spans\n",
                "    )\n",
                "\n",
                "    return predict_text\n",
                "\n",
                "# いくつかの例に対してBERTによる文章校正を行ってみる。\n",
                "text_list = [\n",
                "    'ユーザーの試行に合わせた楽曲を配信する。',\n",
                "    'メールに明日の会議の史料を添付した。',\n",
                "    '乳酸菌で牛乳を発行するとヨーグルトができる。',\n",
                "    '突然、子供が帰省を発した。'\n",
                "]\n",
                "\n",
                "# トークナイザ、ファインチューニング済みのモデルのロード\n",
                "tokenizer = SC_tokenizer.from_pretrained(MODEL_NAME)\n",
                "model = BertForMaskedLM_pl.load_from_checkpoint(best_model_path)\n",
                "bert_mlm = model.bert_mlm.cuda()\n",
                "\n",
                "for text in text_list:\n",
                "    predict_text = predict(text, tokenizer, bert_mlm) # BERTによる予測\n",
                "    print('---')\n",
                "    print(f'入力：{text}')\n",
                "    print(f'出力：{predict_text}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {
                "id": "0zwqdG4SGXNi",
                "jupyter": {
                    "outputs_hidden": false
                }
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 2249/2249 [00:11<00:00, 193.39it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Accuracy: 0.67\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "# 9-17\n",
                "# BERTで予測を行い、正解数を数える。\n",
                "correct_num = 0 \n",
                "for sample in tqdm(dataset_test):\n",
                "    wrong_text = sample['wrong_text']\n",
                "    correct_text = sample['correct_text']\n",
                "    predict_text = predict(wrong_text, tokenizer, bert_mlm) # BERT予測\n",
                "   \n",
                "    if correct_text == predict_text: # 正解の場合\n",
                "        correct_num += 1\n",
                "\n",
                "print(f'Accuracy: {correct_num/len(dataset_test):.2f}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {
                "id": "ZUj8v6dPGXNj"
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 2249/2249 [00:11<00:00, 187.64it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Accuracy: 0.74\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "# 9-18\n",
                "correct_position_num = 0 # 正しく誤変換の漢字を特定できたデータの数\n",
                "for sample in tqdm(dataset_test):\n",
                "    wrong_text = sample['wrong_text']\n",
                "    correct_text = sample['correct_text']\n",
                "    \n",
                "    # 符号化\n",
                "    encoding = tokenizer(wrong_text)\n",
                "    wrong_input_ids = encoding['input_ids'] # 誤変換の文の符合列\n",
                "    encoding = {k: torch.tensor([v]).cuda() for k,v in encoding.items()}\n",
                "    correct_encoding = tokenizer(correct_text)\n",
                "    correct_input_ids = correct_encoding['input_ids'] # 正しい文の符合列\n",
                "    \n",
                "    # 文章を予測\n",
                "    with torch.no_grad():\n",
                "        output = bert_mlm(**encoding)\n",
                "        scores = output.logits\n",
                "        # 予測された文章のトークンのID\n",
                "        predict_input_ids = scores[0].argmax(-1).cpu().numpy().tolist() \n",
                "\n",
                "    # 特殊トークンを取り除く\n",
                "    wrong_input_ids = wrong_input_ids[1:-1]\n",
                "    correct_input_ids =  correct_input_ids[1:-1]\n",
                "    predict_input_ids =  predict_input_ids[1:-1]\n",
                "    \n",
                "    # 誤変換した漢字を特定できているかを判定\n",
                "    # 符合列を比較する。\n",
                "    detect_flag = True\n",
                "    for wrong_token, correct_token, predict_token \\\n",
                "        in zip(wrong_input_ids, correct_input_ids, predict_input_ids):\n",
                "\n",
                "        if wrong_token == correct_token: # 正しいトークン\n",
                "            # 正しいトークンなのに誤って別のトークンに変換している場合\n",
                "            if wrong_token != predict_token: \n",
                "                detect_flag = False\n",
                "                break\n",
                "        else: # 誤変換のトークン\n",
                "            # 誤変換のトークンなのに、そのままにしている場合\n",
                "            if wrong_token == predict_token: \n",
                "                detect_flag = False\n",
                "                break\n",
                "\n",
                "    if detect_flag: # 誤変換の漢字の位置を正しく特定できた場合\n",
                "        correct_position_num += 1\n",
                "        \n",
                "print(f'Accuracy: {correct_position_num/len(dataset_test):.2f}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## モデルとトークナイザの保存"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 68,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "/home/pochi/kouseiyarou-train/model/epoch=1-step=15428.ckpt\n"
                    ]
                }
            ],
            "source": [
                "print(best_model_path)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 69,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
                        "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
                        "The class this function is called from is 'SC_tokenizer'.\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "('models/kouseiyarou_20240721/tokenizer_config.json',\n",
                            " 'models/kouseiyarou_20240721/special_tokens_map.json',\n",
                            " 'models/kouseiyarou_20240721/vocab.txt',\n",
                            " 'models/kouseiyarou_20240721/added_tokens.json')"
                        ]
                    },
                    "execution_count": 69,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import shutil\n",
                "import os\n",
                "new_model_dir = 'models/kouseiyarou_20240721'\n",
                "os.makedirs(new_model_dir, exist_ok=True)\n",
                "new_model_path = new_model_dir + '/model.pth'\n",
                "\n",
                "tokenizer = SC_tokenizer.from_pretrained(MODEL_NAME)\n",
                "tokenizer.save_pretrained(new_model_dir)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 量子化"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 70,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
                        "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
                        "The class this function is called from is 'SC_tokenizer'.\n",
                        "Some weights of the model checkpoint at tohoku-nlp/bert-base-japanese-whole-word-masking were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
                        "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
                        "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "<class '__main__.BertForMaskedLM_pl'>\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "('models/quantize_kouseiyarou_20240721/tokenizer_config.json',\n",
                            " 'models/quantize_kouseiyarou_20240721/special_tokens_map.json',\n",
                            " 'models/quantize_kouseiyarou_20240721/vocab.txt',\n",
                            " 'models/quantize_kouseiyarou_20240721/added_tokens.json')"
                        ]
                    },
                    "execution_count": 70,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "quantize_model_dir = 'models/quantize_kouseiyarou_20240721'\n",
                "quantize_model_path = quantize_model_dir + '/model.pth'\n",
                "\n",
                "# トークナイザ、ファインチューニング済みのモデルのロード\n",
                "tokenizer = SC_tokenizer.from_pretrained(MODEL_NAME)\n",
                "model = BertForMaskedLM_pl.load_from_checkpoint(best_model_path)\n",
                "print(type(model))\n",
                "torch.save(model.to('cpu'), new_model_path)\n",
                "\n",
                "torch.backends.quantized.engine = 'qnnpack'\n",
                "quantized_model = torch.quantization.quantize_dynamic(\n",
                "                model.to('cpu'), {torch.nn.Linear}, dtype=torch.qint8\n",
                "            )\n",
                "\n",
                "torch.save(quantized_model, quantize_model_path)\n",
                "tokenizer.save_pretrained(quantize_model_dir)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 67,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "---\n",
                        "入力：ユーザーの試行に合わせた楽曲を配信する。\n",
                        "出力：ユーザーの嗜好に合わせた楽曲を配信する。\n",
                        "---\n",
                        "入力：メールに明日の会議の史料を添付した。\n",
                        "出力：メールに明日の会議の資料を添付した。\n",
                        "---\n",
                        "入力：乳酸菌で牛乳を発行するとヨーグルトができる。\n",
                        "出力：乳酸菌で牛乳を発酵するとヨーグルトができる。\n",
                        "---\n",
                        "入力：突然、子供が帰省を発した。\n",
                        "出力：突然、子供が帰省を発した。\n"
                    ]
                }
            ],
            "source": [
                "# 9-16\n",
                "def predict_cpu(text, tokenizer, bert_mlm):\n",
                "    \"\"\"\n",
                "    文章を入力として受け、BERTが予測した文章を出力\n",
                "    \"\"\"\n",
                "    # 符号化\n",
                "    encoding, spans = tokenizer.encode_plus_untagged(\n",
                "        text, return_tensors='pt'\n",
                "    ) \n",
                "    encoding = { k: v for k, v in encoding.items() }\n",
                "\n",
                "    # ラベルの予測値の計算\n",
                "    with torch.no_grad():\n",
                "        output = bert_mlm(**encoding)\n",
                "        scores = output.logits\n",
                "        labels_predicted = scores[0].argmax(-1).cpu().numpy().tolist()\n",
                "\n",
                "    # ラベル列を文章に変換\n",
                "    predict_text = tokenizer.convert_bert_output_to_text(\n",
                "        text, labels_predicted, spans\n",
                "    )\n",
                "\n",
                "    return predict_text\n",
                "\n",
                "# いくつかの例に対してBERTによる文章校正を行ってみる。\n",
                "text_list = [\n",
                "    'ユーザーの試行に合わせた楽曲を配信する。',\n",
                "    'メールに明日の会議の史料を添付した。',\n",
                "    '乳酸菌で牛乳を発行するとヨーグルトができる。',\n",
                "    '突然、子供が帰省を発した。'\n",
                "]\n",
                "\n",
                "# トークナイザ、ファインチューニング済みのモデルのロード\n",
                "tokenizer = SC_tokenizer.from_pretrained(quantize_model_dir)\n",
                "#model = torch.load(new_model_path)\n",
                "model = torch.load(quantize_model_path)\n",
                "bert_mlm = model.bert_mlm\n",
                "\n",
                "for text in text_list:\n",
                "    predict_text = predict_cpu(text, tokenizer, bert_mlm) # BERTによる予測\n",
                "    print('---')\n",
                "    print(f'入力：{text}')\n",
                "    print(f'出力：{predict_text}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 61,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "2249\n"
                    ]
                }
            ],
            "source": [
                "print(len(dataset_test))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 66,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        " 14%|█▍        | 319/2249 [14:08<1:25:36,  2.66s/it]\n"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[66], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m wrong_text \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwrong_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m correct_text \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorrect_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 7\u001b[0m predict_text \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_cpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrong_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbert_mlm\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# BERT予測\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m correct_text \u001b[38;5;241m==\u001b[39m predict_text: \u001b[38;5;66;03m# 正解の場合\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     correct_num \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
                        "Cell \u001b[0;32mIn[60], line 14\u001b[0m, in \u001b[0;36mpredict_cpu\u001b[0;34m(text, tokenizer, bert_mlm)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# ラベルの予測値の計算\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 14\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mbert_mlm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     scores \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     16\u001b[0m     labels_predicted \u001b[38;5;241m=\u001b[39m scores[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtolist()\n",
                        "File \u001b[0;32m~/miniconda3/envs/kouseiyarou-train/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/miniconda3/envs/kouseiyarou-train/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
                        "File \u001b[0;32m~/miniconda3/envs/kouseiyarou-train/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1491\u001b[0m, in \u001b[0;36mBertForMaskedLM.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1482\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1483\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;124;03m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;124;03m    config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\u001b[39;00m\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;124;03m    loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1489\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1491\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1497\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1499\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1500\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1505\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1506\u001b[0m prediction_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls(sequence_output)\n",
                        "File \u001b[0;32m~/miniconda3/envs/kouseiyarou-train/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/miniconda3/envs/kouseiyarou-train/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
                        "File \u001b[0;32m~/miniconda3/envs/kouseiyarou-train/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1141\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1141\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1153\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1154\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
                        "File \u001b[0;32m~/miniconda3/envs/kouseiyarou-train/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/miniconda3/envs/kouseiyarou-train/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
                        "File \u001b[0;32m~/miniconda3/envs/kouseiyarou-train/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:694\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    683\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    684\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    685\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    691\u001b[0m         output_attentions,\n\u001b[1;32m    692\u001b[0m     )\n\u001b[1;32m    693\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 694\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    704\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
                        "File \u001b[0;32m~/miniconda3/envs/kouseiyarou-train/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/miniconda3/envs/kouseiyarou-train/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
                        "File \u001b[0;32m~/miniconda3/envs/kouseiyarou-train/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:626\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    623\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    624\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 626\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
                        "File \u001b[0;32m~/miniconda3/envs/kouseiyarou-train/lib/python3.9/site-packages/transformers/pytorch_utils.py:238\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/miniconda3/envs/kouseiyarou-train/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:638\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m--> 638\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    639\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
                        "File \u001b[0;32m~/miniconda3/envs/kouseiyarou-train/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/miniconda3/envs/kouseiyarou-train/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
                        "File \u001b[0;32m~/miniconda3/envs/kouseiyarou-train/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:539\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    538\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[0;32m--> 539\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate_act_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
                        "File \u001b[0;32m~/miniconda3/envs/kouseiyarou-train/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/miniconda3/envs/kouseiyarou-train/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
                        "File \u001b[0;32m~/miniconda3/envs/kouseiyarou-train/lib/python3.9/site-packages/transformers/activations.py:78\u001b[0m, in \u001b[0;36mGELUActivation.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "# 9-17\n",
                "# BERTで予測を行い、正解数を数える。\n",
                "correct_num = 0\n",
                "for sample in tqdm(dataset_test):\n",
                "    wrong_text = sample['wrong_text']\n",
                "    correct_text = sample['correct_text']\n",
                "    predict_text = predict_cpu(wrong_text, tokenizer, bert_mlm) # BERT予測\n",
                "   \n",
                "    if correct_text == predict_text: # 正解の場合\n",
                "        correct_num += 1\n",
                "\n",
                "print(f'Accuracy: {correct_num/len(dataset_test[:100]):.2f}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 9-18\n",
                "correct_position_num = 0 # 正しく誤変換の漢字を特定できたデータの数\n",
                "for sample in tqdm(dataset_test):\n",
                "    wrong_text = sample['wrong_text']\n",
                "    correct_text = sample['correct_text']\n",
                "    \n",
                "    # 符号化\n",
                "    encoding = tokenizer(wrong_text)\n",
                "    wrong_input_ids = encoding['input_ids'] # 誤変換の文の符合列\n",
                "    encoding = {k: torch.tensor([v]).cuda() for k,v in encoding.items()}\n",
                "    correct_encoding = tokenizer(correct_text)\n",
                "    correct_input_ids = correct_encoding['input_ids'] # 正しい文の符合列\n",
                "    \n",
                "    # 文章を予測\n",
                "    with torch.no_grad():\n",
                "        output = bert_mlm(**encoding)\n",
                "        scores = output.logits\n",
                "        # 予測された文章のトークンのID\n",
                "        predict_input_ids = scores[0].argmax(-1).cpu().numpy().tolist() \n",
                "\n",
                "    # 特殊トークンを取り除く\n",
                "    wrong_input_ids = wrong_input_ids[1:-1]\n",
                "    correct_input_ids =  correct_input_ids[1:-1]\n",
                "    predict_input_ids =  predict_input_ids[1:-1]\n",
                "    \n",
                "    # 誤変換した漢字を特定できているかを判定\n",
                "    # 符合列を比較する。\n",
                "    detect_flag = True\n",
                "    for wrong_token, correct_token, predict_token \\\n",
                "        in zip(wrong_input_ids, correct_input_ids, predict_input_ids):\n",
                "\n",
                "        if wrong_token == correct_token: # 正しいトークン\n",
                "            # 正しいトークンなのに誤って別のトークンに変換している場合\n",
                "            if wrong_token != predict_token: \n",
                "                detect_flag = False\n",
                "                break\n",
                "        else: # 誤変換のトークン\n",
                "            # 誤変換のトークンなのに、そのままにしている場合\n",
                "            if wrong_token == predict_token: \n",
                "                detect_flag = False\n",
                "                break\n",
                "\n",
                "    if detect_flag: # 誤変換の漢字の位置を正しく特定できた場合\n",
                "        correct_position_num += 1\n",
                "        \n",
                "print(f'Accuracy: {correct_position_num/len(dataset_test):.2f}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "collapsed_sections": [],
            "name": "Chapter9.ipynb",
            "provenance": [
                {
                    "file_id": "https://github.com/stockmarkteam/bert-book/blob/master/Chapter9.ipynb",
                    "timestamp": 1630577154754
                }
            ]
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.19"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
